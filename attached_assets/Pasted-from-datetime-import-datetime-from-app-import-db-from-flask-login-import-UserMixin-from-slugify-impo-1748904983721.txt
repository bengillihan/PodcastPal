from datetime import datetime
from app import db
from flask_login import UserMixin
from slugify import slugify
import random
import string
import logging

logger = logging.getLogger(__name__)

class User(UserMixin, db.Model):
    id = db.Column(db.Integer, primary_key=True)
    google_id = db.Column(db.String(100), unique=True)
    email = db.Column(db.String(120), unique=True, nullable=False)
    name = db.Column(db.String(100), nullable=False)
    feeds = db.relationship('Feed', backref='owner', lazy=True)

class Feed(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    user_id = db.Column(db.Integer, db.ForeignKey('user.id'), nullable=False)
    name = db.Column(db.String(200), nullable=False)
    description = db.Column(db.Text)
    image_url = db.Column(db.String(500))  # New field for podcast image
    website_url = db.Column(db.String(500))  # New field for optional website URL
    url_slug = db.Column(db.String(200), unique=True, nullable=False)
    created_at = db.Column(db.DateTime, default=datetime.utcnow)
    episodes = db.relationship('Episode', backref='feed', lazy=True)

    __table_args__ = (
        db.Index('ix_feed_user_id', 'user_id'),
        db.Index('ix_feed_url_slug', 'url_slug'),
    )

    def regenerate_url_slug(self):
        """Regenerate the URL slug for the feed"""
        base_slug = slugify(self.name)

        # Add random suffix to ensure uniqueness
        suffix = ''.join(random.choices(string.ascii_lowercase + string.digits, k=6))
        new_slug = f"{base_slug}-{suffix}"

        # Update the slug
        self.url_slug = new_slug
        db.session.commit()

        return new_slug

class Episode(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    feed_id = db.Column(db.Integer, db.ForeignKey('feed.id'), nullable=False)
    title = db.Column(db.String(200), nullable=False)
    description = db.Column(db.Text)
    audio_url = db.Column(db.String(500), nullable=False)
    release_date = db.Column(db.DateTime, nullable=False)
    is_recurring = db.Column(db.Boolean, default=False)
    created_at = db.Column(db.DateTime, default=datetime.utcnow)

    __table_args__ = (
        db.Index('ix_episode_feed_id', 'feed_id'),
        db.Index('ix_episode_release_date', 'release_date'),
    )

class DropboxTraffic(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    date = db.Column(db.Date, nullable=False, unique=True)
    request_count = db.Column(db.Integer, default=0)
    total_bytes = db.Column(db.BigInteger, default=0)
    created_at = db.Column(db.DateTime, default=datetime.utcnow)
    updated_at = db.Column(db.DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    # In-memory batch storage of traffic data
    _batch_data = {}
    _last_batch_update = datetime.utcnow()
    _batch_update_interval = 900  # 15 minutes in seconds

    __table_args__ = (
        db.Index('ix_dropboxtraffic_date', 'date'),
    )

    @classmethod
    def log_request(cls, bytes_transferred=0):
        """Log a Dropbox request with optional bytes transferred"""
        today = datetime.utcnow().date()
        
        # Add to in-memory batch
        if today not in cls._batch_data:
            cls._batch_data[today] = {
                'count': 0,
                'bytes': 0
            }
        
        cls._batch_data[today]['count'] += 1
        cls._batch_data[today]['bytes'] += bytes_transferred
        
        # If it's been more than batch_update_interval since the last update, commit to database
        current_time = datetime.utcnow()
        if (current_time - cls._last_batch_update).total_seconds() > cls._batch_update_interval:
            cls._commit_batch()
            cls._last_batch_update = current_time
    
    @classmethod
    def _commit_batch(cls):
        """Commit the batched traffic data to the database"""
        if not cls._batch_data:
            return
            
        try:
            for date, data in cls._batch_data.items():
                traffic = cls.query.filter_by(date=date).first()
                if not traffic:
                    logger.info(f"Creating new traffic record for date: {date}")
                    traffic = cls(date=date)
                    db.session.add(traffic)

                traffic.request_count += data['count']
                traffic.total_bytes += data['bytes']
                traffic.updated_at = datetime.utcnow()
            
            db.session.commit()
            logger.info(f"Successfully committed batch traffic data for {len(cls._batch_data)} date(s)")
            cls._batch_data = {}  # Clear the batch after committing
        except Exception as e:
            db.session.rollback()
            logger.error(f"Error committing batch traffic data: {e}")
            # Don't raise - we'll try again on the next batch

    @classmethod
    def import_historical_data(cls, date, request_count, total_bytes):
        """Import historical traffic data for a specific date"""
        try:
            traffic = cls.query.filter_by(date=date).first()
            if not traffic:
                traffic = cls(
                    date=date,
                    request_count=request_count,
                    total_bytes=total_bytes,
                    created_at=datetime.utcnow(),
                    updated_at=datetime.utcnow()
                )
                db.session.add(traffic)
                db.session.commit()
                return True
            return False
        except Exception as e:
            db.session.rollback()
            logger.error(f"Error importing historical traffic data: {e}")
            return False